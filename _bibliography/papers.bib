---
---



@inproceedings{yang2010automatic,
  title={Automatic rank determination in projective nonnegative matrix factorization},
  author={Yang, Zhirong and Zhu, Zhanxing and Oja, Erkki},
  booktitle={International Conference on Latent Variable Analysis and Signal Separation},
  pages={514--521},
  year={2010},
  organization={Springer Berlin Heidelberg Berlin, Heidelberg}
}

@article{zhu2013supervised,
abbr={NPL},
  title={Supervised distance preserving projections},
  author={Zhu, Zhanxing and Simil{\"a}, Timo and Corona, Francesco},
  journal={Neural processing letters},
  volume={38},
  pages={445--463},
  year={2013},
  publisher={Springer US}
}

@article{zhu2011local,
abbr={IFAC},
  title={Local linear regression for soft-sensor design with application to an industrial deethanizer},
  author={Zhu, Zhanxing and Corona, Francesco and Lendasse, Amaury and Baratti, Roberto and Romagnoli, Jose A},
  journal={IFAC Proceedings Volumes},
  volume={44},
  number={1},
  pages={2839--2844},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{zhu2013multiplicative,
  title={Multiplicative updates for learning with stochastic matrices},
  author={Zhu, Zhanxing and Yang, Zhirong and Oja, Erkki},
  booktitle={Image Analysis: 18th Scandinavian Conference, SCIA 2013, Espoo, Finland, June 17-20, 2013. Proceedings 18},
  pages={143--152},
  year={2013},
  organization={Springer Berlin Heidelberg}
}

@inproceedings{storkey2014continuum,
  abbr={ICML},
  title={A continuum from mixtures to products: Aggregation under bias},
  author={Storkey, A and Zhu, Zhanxing and Hu, Jinli},
  booktitle={ICML workshop on divergence methods for probabilistic inference},
  year={2014}
}

@inproceedings{zhu2015adaptive,
abbr={ECML},
  title={Adaptive stochastic primal-dual coordinate descent for separable saddle point problems},
  author={Zhu, Zhanxing and Storkey, Amos J},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15},
  pages={645--658},
  year={2015},
  organization={Springer International Publishing}
}

@inproceedings{storkey2015aggregation,
abbr={ECML},
  title={Aggregation under bias: R{\'e}nyi divergence aggregation and its implementation via machine learning markets},
  author={Storkey, Amos J and Zhu, Zhanxing and Hu, Jinli},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15},
  pages={560--574},
  year={2015},
  organization={Springer International Publishing}
}

@inproceedings{shang2015covariance,
abbr={NIPS},
  title={Covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling},
  author={Shang, Xiaocheng and Zhu, Zhanxing and Leimkuhler, Benedict and Storkey, Amos J},
  booktitle={NIPS},
  year={2015}
}

@inproceedings{zhu2016stochastic,
abbr={AAAI},
  title={Stochastic parallel block coordinate descent for large-scale saddle point problems},
  author={Zhu, Zhanxing and Storkey, Amos},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{wu2017towards,
abbr={ICML},
  title={Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes},
  author={Wu, Lei and Zhu, Zhanxing},
  booktitle={The 34th International Conference on Machine Learning (ICML 2017): Theoretical Machine Learning Workshop},
  year={2017}
}

@inproceedings{ye2017langevin,
abbr={NIPS},
  title={Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks},
  author={Ye, Nanyang and Zhu, Zhanxing and Mantiuk, Rafal K},
  booktitle={31st Neural Information Processing Systems (NIPS)},
  year={2017}
}

@inproceedings{yu2018spatio,
abbr={IJCAI},
  title={Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting},
  author={Yu, Bing and Yin, Haoteng and Zhu, Zhanxing},
  booktitle={International Joint Conference of Artificial Intelligence (IJCAI)},
  year={2018},
  pdf={https://www.ijcai.org/proceedings/2018/0505.pdf},
  doi = {https://dl.acm.org/doi/10.5555/3304222.3304273},
  dimensions={true},
  google_scholar_id={fEOibwPWpKIC},
  abstract={Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.},
  award={2019-2024 IJCAI Most Cited Paper},
  selected={true},
}

@inproceedings{xu2018reinforced,
abbr={NIPS},
  title={Reinforced continual learning},
  author={Xu, Ju and Zhu, Zhanxing},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={899--908},
  year={2018}
}

@inproceedings{yu2018tangent,
abbr={CVPR},
  title={Tangent-Normal Adversarial Regularization for Semi-supervised Learning},
  author={Yu, Bing and Wu, Jingfeng and Zhu, Zhanxing},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ye2018yet,
abbr={CCS},
  title={Yet another text captcha solver: A generative adversarial network based approach},
  author={Ye, Guixin and Tang, Zhanyong and Fang, Dingyi and Zhu, Zhanxing and Feng, Yansong and Xu, Pengfei and Chen, Xiaojiang and Wang, Zheng},
  booktitle={Proceedings of the 2018 ACM SIGSAC conference on computer and communications security (ACM CCS)},
  pages={332--348},
  year={2018}
}

@inproceedings{luo2017learning,
abbr={ACL},
  title={Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix},
  author={Luo, Bingfeng and Feng, Yansong and Wang, Zheng and Zhu, Zhanxing and Huang, Songfang and Yan, Rui and Zhao, Dongyan},
  booktitle={ACL},
  year={2017}
}

@inproceedings{yuan2018sipid,
abbr={ISBI},
  title={SIPID: A deep learning framework for sinogram interpolation and image denoising in low-dose CT reconstruction},
  author={Yuan, Huizhuo and Jia, Jinzhu and Zhu, Zhanxing},
  booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI)},
  pages={1521--1524},
  year={2018},
  organization={IEEE}
}

@inproceedings{ye2018stochastic,
abbr={IJCAI},
  title={Stochastic Fractional Hamiltonian Monte Carlo.},
  author={Ye, Nanyang and Zhu, Zhanxing},
  booktitle={IJCAI},
  pages={3019--3025},
  year={2018}
}

@article{luo2018thermostat,
abbr={NIPS},
  title={Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning},
  author={Luo, Rui and Wang, Jianhong and Yang, Yaodong and Wang, Jun and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems (NIPS)},
  volume={31},
  year={2018}
}

@article{ye2018bayesian,
abbr={NIPS},
  title={Bayesian adversarial learning},
  author={Ye, Nanyang and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems (NIPS)},
  volume={31},
  year={2018}
}

@inproceedings{xiong2019sphmc,
abbr={AAAI},
  title={SpHMC: Spectral Hamiltonian Monte Carlo},
  author={Xiong, Haoyi and Wang, Kafeng and Bian, Jiang and Zhu, Zhanxing and Xu, Cheng-Zhong and Guo, Zhishan and Huan, Jun},
  booktitle={AAAI 2019},
  year={2019}
}

@inproceedings{zhang2020learning,
abbr={IJCNN},
  title={Learning to search efficient densenet with layer-wise pruning},
  author={Zhang, Xuanyang and Liu, Hao and Zhu, Zhanxing and Xu, Zenglin},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

@article{zou2019novel,
abbr={Lancet},
  title={Novel subgroups of patients with adult-onset diabetes in Chinese and US populations},
  author={Zou, Xiantong and Zhou, Xianghai and Zhu, Zhanxing and Ji, Linong},
  journal={The Lancet Diabetes \& Endocrinology},
  volume={7},
  number={1},
  pages={9--11},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{sun2019virtual,
abbr={PRCV},
  title={Virtual adversarial training on graph convolutional networks in node classification},
  author={Sun, Ke and Lin, Zhouchen and Guo, Hantao and Zhu, Zhanxing},
  booktitle={Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8--11, 2019, Proceedings, Part I 2},
  pages={431--443},
  year={2019},
  organization={Springer International Publishing}
}

@article{yu20193d,
  title={3D graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting},
  author={Yu, Bing and Li, Mengzhang and Zhang, Jiyong and Zhu, Zhanxing},
  journal={arXiv preprint arXiv:1903.00919},
  year={2019}
}

@article{yu2019st,
  title={ST-UNet: A spatio-temporal U-network for graph-structured time series modeling},
  author={Yu, Bing and Yin, Haoteng and Zhu, Zhanxing},
  journal={arXiv preprint arXiv:1903.05631},
  year={2019}
}

@inproceedings{zhang2019you,
abbr={NeurIPS},
  title={You only propagate once: Accelerating adversarial training via maximal principle},
  author={Zhang, Dinghuai and Zhang, Tianyuan and Lu,  and Zhu, Zhanxing and Dong, Bin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
  abstract={Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximum Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the projected gradient descent (PGD) algorithm.},
  selected={true}
}

@inproceedings{zhang2019interpreting,
abbr={ICML},
  title={Interpreting Adversarially Trained Convolutional Neural Networks},
  author={Zhang, Tianyuan and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@inproceedings{zhu2019anisotropic,
abbr={ICML},
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={7654--7663},
  year={2019},
  pdf={https://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf},
  abstract={Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).},
  selected={true}
}

@inproceedings{wan2020neural,
abbr={ECML},
  title={Neural control variates for Monte Carlo variance reduction},
  author={Wan, Ruosi and Zhong, Mingjun and Xiong, Haoyi and Zhu, Zhanxing},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W{\"u}rzburg, Germany, September 16--20, 2019, Proceedings, Part II},
  pages={533--547},
  year={2020},
  organization={Springer International Publishing}
}

@inproceedings{hu2019question,
abbr={NLPCC},
  title={How question generation can help question answering over knowledge base},
  author={Hu, Sen and Zou, Lei and Zhu, Zhanxing},
  booktitle={Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9--14, 2019, Proceedings, Part I 8},
  pages={80--92},
  year={2019},
  organization={Springer International Publishing}
}

@inproceedings{wan2019towards,
abbr={ICDM},
  title={Towards making deep transfer learning never hurt},
  author={Wan, Ruosi and Xiong, Haoyi and Li, Xingjian and Zhu, Zhanxing and Huan, Jun},
  booktitle={2019 IEEE International Conference on Data Mining (ICDM)},
  pages={578--587},
  year={2019},
  organization={IEEE}
}

@inproceedings{sun2020multi,
abbr={AAAI},
  title={Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes.},
  author={Sun, Ke and Lin, Zhouchen and Zhu, Zhanxing},
  booktitle={AAAI},
  pages={5892--5899},
  year={2020}
}

@inproceedings{yao2020efficient,
abbr={AAAI},
  title={Efficient Neural Architecture Search via Proximal Iterations.},
  author={Yao, Quanming and Xu, Ju and Tu, Wei-Wei and Zhu, Zhanxing},
  booktitle={AAAI},
  pages={6664--6671},
  year={2020}
}

@article{ye2020using,
abbr={TOPS},
  title={Using generative adversarial networks to break and protect text captchas},
  author={Ye, Guixin and Tang, Zhanyong and Fang, Dingyi and Zhu, Zhanxing and Feng, Yansong and Xu, Pengfei and Chen, Xiaojiang and Han, Jungong and Wang, Zheng},
  journal={ACM Transactions on Privacy and Security (TOPS)},
  volume={23},
  number={2},
  pages={1--29},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{wu2020noisy,
abbr= {ICML},
  title={On the Noisy Gradient Descent that Generalizes as SGD},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference for Machine Learning (ICML)},
  year={2020}
}

@inproceedings{ye2020amata,
abbr={AAAI},
  title={Amata: An Annealing Mechanism for Adversarial Training Acceleration},
  author={Ye, Nanyang and Li, Qianxiao and Zhou, Xiao-Yun and Zhu, Zhanxing},
  booktitle={AAAI},
  year={2020}
}

@incollection{guo2020simplifying,
abbr={ECAI},
  title={Simplifying Graph Attention Networks with Source-Target Separation},
  author={Guo, Hantao and Yan, Rui and Feng, Yansong and Gao, Xuesong and Zhu, Zhanxing},
  booktitle={ECAI},
  pages={1166--1173},
  year={2020},
  publisher={IOS Press}
}

@inproceedings{zhang2020black,
abbr={NeurIPS},
  title={Black-box certification with randomized smoothing: A functional optimization based framework},
  author={Zhang, Dinghuai and Ye, Mao and Gong, Chengyue and Zhu, Zhanxing and Liu, Qiang},
  booktitle={NeurIPS},
  year={2020}
}



@inproceedings{shi2020informative,
abbr={ICML},
  title={Informative Dropout for Robust Representation Learning: A Shape-bias Perspective},
  author={Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
  booktitle={International Conference for Machine Learning (ICML)},
  year={2020}
}

@inproceedings{xu2020automatic,
abbr={MICCAI},
  title={Automatic data augmentation for 3D medical image segmentation},
  author={Xu, Ju and Li, Mengzhang and Zhu, Zhanxing},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2020: 23rd International Conference, Lima, Peru, October 4--8, 2020, Proceedings, Part I 23},
  pages={378--387},
  year={2020},
  organization={Springer International Publishing}
}

@inproceedings{wu2020towards,
abbr={ACML},
  title={Towards understanding and improving the transferability of adversarial examples in deep neural networks},
  author={Wu, Lei and Zhu, Zhanxing},
  booktitle={Asian Conference on Machine Learning (ACML)},
  pages={837--850},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2020neural,
abbr={ICLR},
  title={Neural Approximate Sufficient Statistics for Implicit Models},
  author={Chen, Yanzhi and Zhang, Dinghuai and Gutmann, Michael and Courville, Aaron and Zhu, Zhanxing},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2020}
}

@inproceedings{ji2020knowledge,
abbr={NeurIPS},
  title={Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher},
  author={Ji, Guangda and Zhu, Zhanxing},
  booktitle={Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)},
  year={2020},
  pdf={https://proceedings.neurips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf}
}

@inproceedings{chen2020breaking,
abbr={ICML},
  title={On breaking deep generative model-based defenses and beyond},
  author={Chen, Yanzhi and Xie, Renjie and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1736--1745},
  year={2020},
  organization={PMLR}
}

@ARTICLE{wang2021,
abbr={TVCG},
  author={Wang, He and Ho, Edmond S. L. and Shum, Hubert P. H. and Zhu, Zhanxing},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Spatio-Temporal Manifold Learning for Human Motions via Long-Horizon Modeling}, 
  year={2021},
  volume={27},
  number={1},
  pages={216-227},
  keywords={Manifolds;Deep learning;Skeleton;Three-dimensional displays;Feature extraction;Dynamics;Animation;Computer graphics;computer animation;character animation;deep learning},
  doi={10.1109/TVCG.2019.2936810}
  }


@article{wang2021sampling,
abbr={TKDD},
  title={Sampling sparse representations with randomized measurement langevin dynamics},
  author={Wang, Kafeng and Xiong, Haoyi and Bian, Jiang and Zhu, Zhanxing and Gao, Qian and Guo, Zhishan and Xu, Cheng-Zhong and Huan, Jun and Dou, Dejing},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={15},
  number={2},
  pages={1--21},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{xie2021positive,
abbr={ICML},
  title={Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization},
  author={Xie, Zeke and Yuan, Li and Zhu, Zhanxing and Sugiyama, Masashi},
  booktitle={International Conference for Machine Learning (ICML)},
  year={2021}
}

@inproceedings{sun2021adagcn,
abbr={ICLR},
  title={AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models},
  author={Sun, Ke and Zhu, Zhanxing and Lin, Zhouchen},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2021}
}

@inproceedings{ye2021adversarial,
abbr={CVPR},
  title={Adversarial invariant learning},
  author={Ye, Nanyang and Tang, Jingxuan and Deng, Huayu and Zhou, Xiao-Yun and Li, Qianxiao and Li, Zhenguo and Yang, Guang-Zhong and Zhu, Zhanxing},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12441--12449},
  year={2021},
  organization={IEEE}
}

@inproceedings{li2020spatial,
abbr={AAAI},
  title={Spatial-temporal fusion graph neural networks for traffic flow forecasting},
  author={Li, Mengzhang and Zhu, Zhanxing},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020},
  dimensions={true}
}

@article{xu2021adaptive,
abbr={TPAMI},
  title={Adaptive progressive continual learning},
  author={Xu, Ju and Ma, Jin and Gao, Xuesong and Zhu, Zhanxing},
  journal={IEEE transactions on pattern analysis and machine intelligence ({TPAMI})},
  volume={44},
  number={10},
  pages={6715--6728},
  year={2021},
  publisher={IEEE}
}

@article{ye2021annealing,
abbr={TNNLS},
  title={An annealing mechanism for adversarial training acceleration},
  author={Ye, Nanyang and Li, Qianxiao and Zhou, Xiao-Yun and Zhu, Zhanxing},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
   publisher={IEEE}
}

@article{wan2021spherical,
abbr={NeurIPS},
  title={{Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay}},
  author={Wan, Ruosi and Zhu, Zhanxing and Zhang, Xiangyu and Sun, Jian},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  pages={6380--6391},
  year={2021},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf},
  abstract={In this paper, we comprehensively reveal the learning dynamics of normalized neural network using Stochastic Gradient Descent (with momentum) and Weight Decay (WD), named as Spherical Motion Dynamics (SMD). Most related works focus on studying behavior of effective learning rate "inequilibrium" state, i.e. assuming weight norm remains unchanged. However, their discussion on why this equilibrium can be reached is either absent or less convincing. Our work directly explores the cause of equilibrium, as a special state of SMD. Specifically, 1) we introduce the assumptions that can lead to equilibrium state in SMD, and prove equilibrium can be reached in a linear rate regime under given assumptions; 2) we propose 
angular update" as a substitute for effective learning rate to depict the state of SMD, and derive the theoretical value of angular update in equilibrium state; 3) we verify our assumptions and theoretical results on various large-scale computer vision tasks including ImageNet and MSCOCO with standard settings. Experiment results show our theoretical findings agree well with empirical observations. We also show that the behavior of angular update in SMD can produce interesting effect to the optimization of neural network in practice.},
  selected={true}
}

@inproceedings{gong2022fine,
abbr={ICLR},
  title={Fine-grained differentiable physics: a yarn-level model for fabrics},
  author={Gong, Deshan and Zhu, Zhanxing and Bulpitt, Andrew J and Wang, He},
  booktitle={International Conference on Learning Representation (ICLR)},
  pdf={https://openreview.net/forum?id=KPEFXR1HdIo},
  year={2022}
}

@inproceedings{lv2022implicit,
abbr={ICLR},
  title={Implicit Bias of Adversarial Training for Deep Neural Networks},
  author={Lv, Bochen and Zhu, Zhanxing},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2022},
  pdf={https://openreview.net/pdf?id=l8It-0lE5e7},
  abstract={We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with  l2 or l-infinity FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.},
  selected={false}
}

@article{xiong2022grod,
abbr={TKDD},
  title={Grod: Deep learning with gradients orthogonal decomposition for knowledge transfer, distillation, and adversarial training},
  author={Xiong, Haoyi and Wan, Ruosi and Zhao, Jian and Chen, Zeyu and Li, Xingjian and Zhu, Zhanxing and Huan, Jun},
  journal={ACM Transactions on Knowledge Discovery from Data},
  volume={16},
  number={6},
  pages={1--25},
  year={2022},
  publisher={ACM New York, NY}
}



@inproceedings{yi2023monoflow,
abbr={ICML},
  title={MonoFlow: Rethinking Divergence GANs via the Perspective of Differential Equations},
  author={Yi, Mingxuan and Zhu, Zhanxing and Liu, Song},
  booktitle={International Conference for Machine Learning (ICML)},
  year={2023},
  pdf={https://proceedings.mlr.press/v202/yi23c.html}
}

@inproceedings{li2023neural,
abbr={NeurIPS},
  title={Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling},
  author={Li, Ting and Li, Jianguo and Zhu, Zhanxing},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023},
  pdf ={https://openreview.net/pdf?id=bISkJSa5Td},
}

@inproceedings{lyu2023implicit,
abbr={NeurIPS},
  title={Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network},
  author={Lyu, Bochen and Zhu, Zhanxing},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)},
  year={2023},
  pdf = {https://openreview.net/pdf?id=PjBEUTVzoe},
  dimensions={true},
  abstract={Studying the implicit bias of gradient descent (GD) and stochastic gradient descent (SGD) is critical to unveil the underlying mechanism of deep learning. Unfortunately, even for standard linear networks in regression setting, a comprehensive characterization of the implicit bias is still an open problem. This paper proposes to investigate a new proxy model of standard linear network, rank-1 linear network, where each weight matrix is parameterized as a rank-1 form. For over-parameterized regression problem, we precisely analyze the implicit bias of GD and SGD---by identifying a “potential” function such that GD converges to its minimizer constrained by zero training error (i.e., interpolation solution), and further characterizing the role of the noise introduced by SGD in perturbing the form of this potential. Our results explicitly connect the depth of the network and the initialization with the implicit bias of GD and SGD. Furthermore, we emphasize a new implicit bias of SGD jointly induced by stochasticity and over-parameterization, which can reduce the dependence of the SGD's solution on the initialization. Our findings regarding the implicit bias are different from that of a recently popular model, the diagonal linear network. We highlight that the induced bias of our rank-1 model is more consistent with standard linear network while the diagonal one is not. This suggests that the proposed rank-1 linear network might be a plausible proxy for standard linear net.},
  selected={true}
}




@article{xiong2023stochastic,
  abbr={MLST},
  title={Stochastic Gradient Descent with Random Label Noises: Doubly Stochastic Models and Inference Stabilizer},
  author={Xiong, Haoyi and Li, Xuhong and Yu, Boyang and Wu, Dongrui and Zhu, Zhanxing and Dou, Dejing},
  journal={Machine Learning: Science and Technology},
  pdf = {https://iopscience.iop.org/article/10.1088/2632-2153/ad13ba},
  year={2023}
}

@inproceedings{sun2023patch,
  abbr={ACML},
  title={Patch-level neighborhood interpolation: A general and effective graph-based regularization strategy},
  author={Sun, Ke and Yu, Bing and Lin, Zhouchen and Zhu, Zhanxing},
  booktitle={Asian Conference on Machine Learning (ACML)},
  pages={1276--1291},
  year={2023},
}


@inproceedings{shen2024memory,
abbr={NeurIPS},
  title={Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization},
  author={Shen, Qianli and Wang, Yezhen and Yang, Zhouhao and Li, Xiang and Wang, Haonan and Zhang, Yang and Scarlett, Jonathan and Zhu, Zhanxing and Kawaguchi, Kenji},
  booktitle={Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024},
  pdf = {https://openreview.net/forum?id=MI8Z9gutIn&noteId=Lot1chBI53}
}


@article{li2024functional,
abbr={AI Journal},
title = {Functional Relation Field: A Model-Agnostic Framework for Multivariate Time Series Forecasting},
journal = {Artificial Intelligence},
volume = {334},
pages = {104158},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104158},
pdf = {https://www.sciencedirect.com/science/article/pii/S0004370224000948},
author = {Ting Li and Bing Yu and Jianguo Li and Zhanxing Zhu},
abstract = {In multivariate time series forecasting, the most popular strategy for modeling the relationship between multiple time series is the construction of graph, where each time series is represented as a node and related nodes are connected by edges. However, the relationship between multiple time series is typically complicated, e.g. the sum of outflows from upstream nodes may be equal to the inflows of downstream nodes. Such relations widely exist in many real-world scenarios for multivariate time series forecasting, yet are far from well studied. In these cases, graph might be insufficient for modeling the complex dependency between nodes. To this end, we explore a new framework to model the inter-node relationship in a more precise way based our proposed inductive bias, Functional Relation Field, where a group of functions parameterized by neural networks are learned to characterize the dependency between multiple time series. Essentially, these learned functions then form a “field”, i.e. a particular set of constraints, to regularize the training loss of the backbone prediction network and enforce the inference process to satisfy these constraints. Since our framework introduces the relationship bias in a data-driven manner, it is flexible and model-agnostic such that it can be applied to any existing multivariate time series prediction networks for boosting performance. The experiment is conducted on one toy dataset to show our approach can well recover the true constraint relationship between nodes. And various real-world datasets are also considered with different backbone prediction networks. Results show that the prediction error can be reduced remarkably with the aid of the proposed framework.}
}




@article{he2024foodie,
abbr={PNAS},
author = {Runsheng He  and Wenyang Dong  and Zhi Wang  and Chen Xie  and Long Gao  and Wenping Ma  and Ke Shen  and Dubai Li  and Yuxuan Pang  and Fanchong Jian  and Jiankun Zhang  and Yuan Yuan  and Xinyao Wang  and Zhen Zhang  and Yinghui Zheng  and Shuang Liu  and Cheng Luo  and Xiaoran Chai  and Jun Ren  and Zhanxing Zhu  and Xiaoliang Sunney Xie },
title = {Genome-wide single-cell and single-molecule footprinting of transcription factors with deaminase},
journal = {Proceedings of the National Academy of Sciences},
volume = {121},
number = {52},
year = {2024},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2423270121},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2423270121},
abstract = {An individual’s somatic cells have essentially the same genome, but each cell type is determined by combinations of transcription factors (TFs) bound to each gene’s regulatory regions, controlling the transcription of DNA into RNA. Investigations of TFs have come from either “bottom–up” or “top–down” approaches. Bottom–up approaches start at the molecular level, including atomic resolution structures and single-molecule imaging of protein–DNA complexes. “Top–down” approaches start at the whole-organism or whole-cell level, including classic genetic studies and molecular biology. Understanding functional genomics requires a holistic approach to combine molecular-, cellular-, and tissue-level studies of TFs. Here, we report a technique that allowes genome-wide studies of TF binding on a single-molecule and single-cell basis. Decades of research have established that mammalian transcription factors (TFs) bind to each gene’s regulatory regions and cooperatively control tissue specificity, timing, and intensity of gene transcription. Mapping the combination of TF binding sites genome wide is critically important for understanding functional genomics. Here, we report a technique to measure TFs’ binding sites on the human genome with a near single-base resolution by footprinting with deaminase (FOODIE) on a single-molecule and single-cell basis. Single-molecule sequencing reads after enzymatic deamination allow detection of the TF binding fraction on a particular footprint and the binding cooperativity of any two adjacent TFs, which can be either positive or negative. As a newcomer of single-cell genomics, single-cell FOODIE enables the detection of cell-type-specific TF footprints in a pure cell population in a heterogeneous tissue, such as the brain. We found that genes carrying out a certain biological function together in a housing-keeping correlated gene module (CGM) or a tissues-specific CGM are coordinated by shared TFs in the gene’s promoters and enhancers, respectively. Scalable and cost-effective, FOODIE allows us to create an open FOODIE database for cell lines, with applicability to human tissues and clinical samples.}
}


@article{lyu2025analyzing,
abbr={TPAMI},
  title={Analyzing the implicit bias of adversarial training from a generalized margin perspective},
  author={Lyu, Bochen and Zhu, Zhanxing},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence ({TPAMI})},
  %volume={44},
  %number={10},
  %pages={6715--6728},
  year={2025},
  publisher={IEEE},
  abstract = {Adversarial training has been empirically demonstrated as an effective strategy to improve the robustness of deep neural networks (DNNs) against adversarial examples. However, the underlying reason of its effectiveness is still non-transparent. In this paper we conduct both extensive theoretical and empirical analysis on the implicit bias induced by adversarial training from a generalized margin perspective. 

Our results focus on adversarial training for homogeneous DNNs. In particular, (i). For deep linear networks with $\ell_p$-norm perturbation, we show that weight matrices of adjacent layers get aligned and the converged parameters maximize the margin of adversarial examples, which can be further viewed as a generalized margin of the original dataset that can be achieved by an interpolation solution between $\ell_2$-SVM and $\ell_q$-SVM where $1/p + 1/q=1$. 
(ii). For general homogeneous DNNs, {\color{black} including both linear and nonlinear ones,} we investigate adversarial training with a variety of adversarial perturbations in a unified manner. Specifically, we show that the direction of the limit point of parameters converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. {\color{black}Additionally, as an application of this general result for two special linear homogeneous DNNs, diagonal linear networks and linear convolutional networks, we show that adversarial training with $\ell_p$-norm perturbation equivalently minimizes an interpolation norm that depends on the depth, the architecture, and the value of $p$ in the predictor space. }

Extensive experiments are conducted to verify theoretical claims. Our results theoretically provide the basis for the longstanding folklore \cite{AT} that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.},
selected={true}
}


@inproceedings{tao2025unisoma,
abbr={ICML},
title={Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems},
author={Tao, Shilong and Feng, Zhe and Sun, Haonan and Zhu, Zhanxing and Liu, Yunhuai},
booktitle={International Conference for Machine Learning (ICML)},
year={2025},
abstract = {Multi-solid systems are foundational to a wide range of real-world applications, yet modeling their complex interactions remains challenging. Existing deep learning methods predominantly rely on implicit modeling, where the factors influencing solid deformation are not explicitly represented but are instead indirectly learned. However, as the number of solids increases, these methods struggle to accurately capture intricate physical interactions. In this paper, we introduce a novel explicit modeling paradigm that incorporates factors influencing solid deformation through structured modules. Specifically, we present Unisoma, a unified and flexible Transformer-based model capable of handling variable numbers of solids. Unisoma directly captures physical interactions using contact modules and adaptive interaction allocation mechanism, and learns the deformation through a triplet relationship. Compared to implicit modeling techniques, explicit modeling is more well-suited for multi-solid systems with diverse coupling patterns, as it enables detailed treatment of each solid while preventing information blending and confusion. Experimentally, Unisoma achieves consistent state-of-the-art performance across seven well-established datasets and two complex multi-solid tasks.},
selected={false}
}


@inproceedings{tao2025ladeep,
abbr={KDD},
title={LaDEEP: A Deep Learning-based Surrogate Model for Large Deformation of Elastic-Plastic Solids},
author={Tao, Shilong and Feng, Zhe and Sun, Haonan and Zhu, Zhanxing and Liu, Yunhuai},
booktitle={31st SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) - Applied Data Science Track},
year={2025},
abstract = {Scientific computing for large deformation of elastic-plastic solids is critical for numerous real-world applications. Classical numerical solvers rely primarily on local discrete linear approximation and are constrained by an inherent trade-off between accuracy and efficiency. Recently, deep learning models have achieved impressive progress in solving the continuum mechanism. While previous models have explored various architectures and constructed coefficient-solution mappings, they are designed for general instances without considering specific problem properties and hard to accurately handle with complex elastic-plastic solids involving contact, loading and unloading. In this work, we take stretch bending, a popular metal fabrication technique, as our case study and introduce LaDEEP, a deep learning-based surrogate model for \textbf{La}rge \textbf{De}formation of \textbf{E}lastic-\textbf{P}lastic Solids. We encode the partitioned regions of the involved slender solids into a token sequence to maintain their essential order property. To characterize the physical process of the solid deformation, a two-stage Transformer-based module is designed to predict the deformation with the sequence of tokens as input. Empirically, LaDEEP achieves five magnitudes faster speed than finite element methods with a comparable accuracy, and gains 20.47% relative improvement on average compared to other deep learning baselines. We have also deployed our model into a real-world industrial production system, and it has shown remarkable performance in both accuracy and efficiency.},
selected={false}
}

@inproceedings{lyu2025solvable,
abbr={ICLR},
  title={{A Solvable Attention for Neural Scaling Laws}},
  author={Lyu, Bochen and Wang, Di and Zhu, Zhanxing},
  booktitle={International Conference on Learning Representation (ICLR)},
  pdf={https://openreview.net/forum?id=wYxOMEzpkl},
  year={2025},
  abstract = {Transformers and many other deep learning models are empirically shown to predictably enhance their performance as a power law in training time, model size, or the number of training data points, which is termed as the neural scaling law. This paper studies this intriguing phenomenon particularly for the transformer architecture in theoretical setups. Specifically, we propose a framework for self-attention, the underpinning block of transformer, to learn in an in-context manner, where the corresponding learning dynamics is modeled as a non-linear ordinary differential equation (ODE) system. Furthermore, we establish a procedure to derive a tractable solution for this ODE system by reformulating it as a Riccati equation, which allows us to precisely characterize neural scaling laws for self-attention with training time, model size, data size, and the optimal compute. In addition, we reveal that the self-attention shares similar neural scaling laws with several other architectures when the context sequence length of the in-context learning is fixed, otherwise it would exhibit a different scaling law of training time.},
  selected={true}
}




@inproceedings{cheng2025dycast,
abbr={ICLR},
  title={{DyCAST: Learning Dynamic Causal Structure from Time Series}},
  author={Cheng, Yue and Lyu, Bochen and Xing, Weiwei and Zhu, Zhanxing},
  booktitle={International Conference on Learning Representation (ICLR)},
  pdf={https://openreview.net/forum?id=WjDjem8mWE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)},
  year={2025},
   abstract = {Understanding the dynamics of causal structures is crucial for uncovering the underlying processes in time series data. Previous approaches rely on static assumptions, where contemporaneous and time-lagged dependencies are assumed to have invariant topological structures. However, these models fail to capture the evolving causal relationship between variables when the underlying process exhibits such dynamics. To address this limitation, we propose DyCAST, a novel framework designed to learn dynamic causal structures in time series using Neural Ordinary Differential Equations (Neural ODEs). The key innovation lies in modeling the temporal dynamics of the contemporaneous structure, drawing inspiration from recent advances in Neural ODEs on constrained manifolds. We reformulate the task of learning causal structures at each time step as solving the solution trajectory of a Neural ODE on the directed acyclic graph (DAG) manifold. To accommodate high-dimensional causal structures, we extend DyCAST by learning the temporal dynamics of the hidden state for contemporaneous causal structure. Experiments on both synthetic and real-world datasets demonstrate that DyCAST achieves superior or comparable performance compared to existing causal discovery models.},
  selected={false}
}

@inproceedings{lyu2025effects,
abbr={AAAI},
title={Effects of Momentum in Implicit Bias of Gradient Flow for Diagonal Linear Networks},
author={Lyu, Bochen and Wang, He and Wang, Zheng and Zhu, Zhanxing},
booktitle={The 39th Annual AAAI Conference on Artificial Intelligence (AAAI)},
pdf ={https://ojs.aaai.org/index.php/AAAI/article/view/34118},
year={2025},
abstract = {This paper targets on the regularization effect of momentum-based methods in regression settings and analyzes the popular diagonal linear networks to precisely characterize the implicit bias of continuous versions of heavy-ball (HB) and Nesterov's method of accelerated gradients (NAG). We show that, HB and NAG exhibit different implicit bias compared to GD for diagonal linear networks, which is different from the one for classic linear regression problem where momentum-based methods share the same implicit bias with GD. Specifically, the role of momentum in the implicit bias of GD is twofold: (a) HB and NAG induce extra initialization mitigation effects similar to SGD that are beneficial for generalization of sparse regression; (b) the implicit regularization effects of HB and NAG also depend on the initialization of gradients explicitly, which may not be benign for generalization. As a result, whether HB and NAG have better generalization properties than GD jointly depends on the aforementioned twofold effects determined by various parameters such as learning rate, momentum factor, and integral of gradients. Our findings highlight the potential beneficial role of momentum and can help understand its advantages in practice such as when it will lead to better generalization performance.}, 
selected={false}
}

@ARTICLE{li2025,
abbr={TCSVT},
  author={Li, Ruochen and Qiao, Tanqiu and Katsigiannis, Stamos and Zhu, Zhanxing and Shum, Hubert P. H.},
  journal={IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)}, 
  title={{Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction}}, 
  year={2025},
  pages={1-1},
  doi={10.1109/TCSVT.2025.3539522},
  selected={false}
  }







