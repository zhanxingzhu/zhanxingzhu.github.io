<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Zhanxing Zhu </title> <meta name="author" content="Zhanxing Zhu"> <meta name="description" content="publications by categories in reversed chronological order."> <meta name="keywords" content="machine learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhanxingzhu.github.io//publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhanxing Zhu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">group members </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="lyu2025solvable" class="col-sm-8"> <div class="title">A Solvable Attention for Neural Scaling Laws</div> <div class="author"> Bochen Lyu ,  Di Wang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=wYxOMEzpkl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Transformers and many other deep learning models are empirically shown to predictably enhance their performance as a power law in training time, model size, or the number of training data points, which is termed as the neural scaling law. This paper studies this intriguing phenomenon particularly for the transformer architecture in theoretical setups. Specifically, we propose a framework for self-attention, the underpinning block of transformer, to learn in an in-context manner, where the corresponding learning dynamics is modeled as a non-linear ordinary differential equation (ODE) system. Furthermore, we establish a procedure to derive a tractable solution for this ODE system by reformulating it as a Riccati equation, which allows us to precisely characterize neural scaling laws for self-attention with training time, model size, data size, and the optimal compute. In addition, we reveal that the self-attention shares similar neural scaling laws with several other architectures when the context sequence length of the in-context learning is fixed, otherwise it would exhibit a different scaling law of training time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="cheng2025dycast" class="col-sm-8"> <div class="title">DyCAST: Learning Dynamic Causal Structure from Time Series</div> <div class="author"> Yue Cheng ,  Bochen Lyu ,  Weiwei Xing ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=WjDjem8mWE&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Understanding the dynamics of causal structures is crucial for uncovering the underlying processes in time series data. Previous approaches rely on static assumptions, where contemporaneous and time-lagged dependencies are assumed to have invariant topological structures. However, these models fail to capture the evolving causal relationship between variables when the underlying process exhibits such dynamics. To address this limitation, we propose DyCAST, a novel framework designed to learn dynamic causal structures in time series using Neural Ordinary Differential Equations (Neural ODEs). The key innovation lies in modeling the temporal dynamics of the contemporaneous structure, drawing inspiration from recent advances in Neural ODEs on constrained manifolds. We reformulate the task of learning causal structures at each time step as solving the solution trajectory of a Neural ODE on the directed acyclic graph (DAG) manifold. To accommodate high-dimensional causal structures, we extend DyCAST by learning the temporal dynamics of the hidden state for contemporaneous causal structure. Experiments on both synthetic and real-world datasets demonstrate that DyCAST achieves superior or comparable performance compared to existing causal discovery models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="lyu2025effects" class="col-sm-8"> <div class="title">Effects of Momentum in Implicit Bias of Gradient Flow for Diagonal Linear Networks</div> <div class="author"> Bochen Lyu ,  He Wang ,  Zheng Wang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In The 39th Annual AAAI Conference on Artificial Intelligence</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="shen2024memory" class="col-sm-8"> <div class="title">Memory-Efficient Gradient Unrolling for Large-Scale Bi-level Optimization</div> <div class="author"> Qianli Shen ,  Yezhen Wang ,  Zhouhao Yang ,  Xiang Li ,  Haonan Wang ,  Yang Zhang , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jonathan Scarlett, Zhanxing Zhu, Kenji Kawaguchi' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=MI8Z9gutIn&amp;noteId=Lot1chBI53" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AI Journal</abbr> </div> <div id="li2024functional" class="col-sm-8"> <div class="title">Functional Relation Field: A Model-Agnostic Framework for Multivariate Time Series Forecasting</div> <div class="author"> Ting Li ,  Bing Yu ,  Jianguo Li ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>Artificial Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0004370224000948" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In multivariate time series forecasting, the most popular strategy for modeling the relationship between multiple time series is the construction of graph, where each time series is represented as a node and related nodes are connected by edges. However, the relationship between multiple time series is typically complicated, e.g. the sum of outflows from upstream nodes may be equal to the inflows of downstream nodes. Such relations widely exist in many real-world scenarios for multivariate time series forecasting, yet are far from well studied. In these cases, graph might be insufficient for modeling the complex dependency between nodes. To this end, we explore a new framework to model the inter-node relationship in a more precise way based our proposed inductive bias, Functional Relation Field, where a group of functions parameterized by neural networks are learned to characterize the dependency between multiple time series. Essentially, these learned functions then form a “field”, i.e. a particular set of constraints, to regularize the training loss of the backbone prediction network and enforce the inference process to satisfy these constraints. Since our framework introduces the relationship bias in a data-driven manner, it is flexible and model-agnostic such that it can be applied to any existing multivariate time series prediction networks for boosting performance. The experiment is conducted on one toy dataset to show our approach can well recover the true constraint relationship between nodes. And various real-world datasets are also considered with different backbone prediction networks. Results show that the prediction error can be reduced remarkably with the aid of the proposed framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">PNAS</abbr> </div> <div id="he2024foodie" class="col-sm-8"> <div class="title">Genome-wide single-cell and single-molecule footprinting of transcription factors with deaminase</div> <div class="author"> Runsheng He ,  Wenyang Dong ,  Zhi Wang ,  Chen Xie ,  Long Gao ,  Wenping Ma , and <span class="more-authors" title="click to view 15 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '15 more authors' ? 'Ke Shen, Dubai Li, Yuxuan Pang, Fanchong Jian, Jiankun Zhang, Yuan Yuan, Xinyao Wang, Zhen Zhang, Yinghui Zheng, Shuang Liu, Cheng Luo, Xiaoran Chai, Jun Ren, Zhanxing Zhu, Xiaoliang Sunney Xie' : '15 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">15 more authors</span> </div> <div class="periodical"> <em>Proceedings of the National Academy of Sciences</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>An individual’s somatic cells have essentially the same genome, but each cell type is determined by combinations of transcription factors (TFs) bound to each gene’s regulatory regions, controlling the transcription of DNA into RNA. Investigations of TFs have come from either “bottom–up” or “top–down” approaches. Bottom–up approaches start at the molecular level, including atomic resolution structures and single-molecule imaging of protein–DNA complexes. “Top–down” approaches start at the whole-organism or whole-cell level, including classic genetic studies and molecular biology. Understanding functional genomics requires a holistic approach to combine molecular-, cellular-, and tissue-level studies of TFs. Here, we report a technique that allowes genome-wide studies of TF binding on a single-molecule and single-cell basis. Decades of research have established that mammalian transcription factors (TFs) bind to each gene’s regulatory regions and cooperatively control tissue specificity, timing, and intensity of gene transcription. Mapping the combination of TF binding sites genome wide is critically important for understanding functional genomics. Here, we report a technique to measure TFs’ binding sites on the human genome with a near single-base resolution by footprinting with deaminase (FOODIE) on a single-molecule and single-cell basis. Single-molecule sequencing reads after enzymatic deamination allow detection of the TF binding fraction on a particular footprint and the binding cooperativity of any two adjacent TFs, which can be either positive or negative. As a newcomer of single-cell genomics, single-cell FOODIE enables the detection of cell-type-specific TF footprints in a pure cell population in a heterogeneous tissue, such as the brain. We found that genes carrying out a certain biological function together in a housing-keeping correlated gene module (CGM) or a tissues-specific CGM are coordinated by shared TFs in the gene’s promoters and enhancers, respectively. Scalable and cost-effective, FOODIE allows us to create an open FOODIE database for cell lines, with applicability to human tissues and clinical samples.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="yi2023monoflow" class="col-sm-8"> <div class="title">MonoFlow: Rethinking Divergence GANs via the Perspective of Differential Equations</div> <div class="author"> Mingxuan Yi ,  <em>Zhanxing Zhu</em> ,  and  Song Liu </div> <div class="periodical"> <em>In International Conference for Machine Learning (ICML)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.mlr.press/v202/yi23c.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="li2023neural" class="col-sm-8"> <div class="title">Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling</div> <div class="author"> Ting Li ,  Jianguo Li ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/pdf?id=bISkJSa5Td" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="lyu2023implicit" class="col-sm-8"> <div class="title">Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network</div> <div class="author"> Bochen Lyu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=PjBEUTVzoe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Studying the implicit bias of gradient descent (GD) and stochastic gradient descent (SGD) is critical to unveil the underlying mechanism of deep learning. Unfortunately, even for standard linear networks in regression setting, a comprehensive characterization of the implicit bias is still an open problem. This paper proposes to investigate a new proxy model of standard linear network, rank-1 linear network, where each weight matrix is parameterized as a rank-1 form. For over-parameterized regression problem, we precisely analyze the implicit bias of GD and SGD—by identifying a “potential” function such that GD converges to its minimizer constrained by zero training error (i.e., interpolation solution), and further characterizing the role of the noise introduced by SGD in perturbing the form of this potential. Our results explicitly connect the depth of the network and the initialization with the implicit bias of GD and SGD. Furthermore, we emphasize a new implicit bias of SGD jointly induced by stochasticity and over-parameterization, which can reduce the dependence of the SGD’s solution on the initialization. Our findings regarding the implicit bias are different from that of a recently popular model, the diagonal linear network. We highlight that the induced bias of our rank-1 model is more consistent with standard linear network while the diagonal one is not. This suggests that the proposed rank-1 linear network might be a plausible proxy for standard linear net.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MLST</abbr> </div> <div id="xiong2023stochastic" class="col-sm-8"> <div class="title">Stochastic Gradient Descent with Random Label Noises: Doubly Stochastic Models and Inference Stabilizer</div> <div class="author"> Haoyi Xiong ,  Xuhong Li ,  Boyang Yu ,  Dongrui Wu ,  <em>Zhanxing Zhu</em> ,  and  Dejing Dou </div> <div class="periodical"> <em>Machine Learning: Science and Technology</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://iopscience.iop.org/article/10.1088/2632-2153/ad13ba" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ACML</abbr> </div> <div id="sun2023patch" class="col-sm-8"> <div class="title">Patch-level neighborhood interpolation: A general and effective graph-based regularization strategy</div> <div class="author"> Ke Sun ,  Bing Yu ,  Zhouchen Lin ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Asian Conference on Machine Learning (ACML)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="gong2022fine" class="col-sm-8"> <div class="title">Fine-grained differentiable physics: a yarn-level model for fabrics</div> <div class="author"> Deshan Gong ,  <em>Zhanxing Zhu</em> ,  Andrew J Bulpitt ,  and  He Wang </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://openreview.net/forum?id=KPEFXR1HdIo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="lv2022implicit" class="col-sm-8"> <div class="title">Implicit Bias of Adversarial Training for Deep Neural Networks</div> <div class="author"> Bochen Lv ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=l8It-0lE5e7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with l2 or l-infinity FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TKDD</abbr> </div> <div id="xiong2022grod" class="col-sm-8"> <div class="title">Grod: Deep learning with gradients orthogonal decomposition for knowledge transfer, distillation, and adversarial training</div> <div class="author"> Haoyi Xiong ,  Ruosi Wan ,  Jian Zhao ,  Zeyu Chen ,  Xingjian Li ,  <em>Zhanxing Zhu</em> , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jun Huan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>ACM Transactions on Knowledge Discovery from Data</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TVCG</abbr> </div> <div id="wang2021" class="col-sm-8"> <div class="title">Spatio-Temporal Manifold Learning for Human Motions via Long-Horizon Modeling</div> <div class="author"> He Wang ,  Edmond S. L. Ho ,  Hubert P. H. Shum ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TKDD</abbr> </div> <div id="wang2021sampling" class="col-sm-8"> <div class="title">Sampling sparse representations with randomized measurement langevin dynamics</div> <div class="author"> Kafeng Wang ,  Haoyi Xiong ,  Jiang Bian ,  <em>Zhanxing Zhu</em> ,  Qian Gao ,  Zhishan Guo , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Cheng-Zhong Xu, Jun Huan, Dejing Dou' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ACM Transactions on Knowledge Discovery from Data (TKDD)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="xie2021positive" class="col-sm-8"> <div class="title">Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization</div> <div class="author"> Zeke Xie ,  Li Yuan ,  <em>Zhanxing Zhu</em> ,  and  Masashi Sugiyama </div> <div class="periodical"> <em>In International Conference for Machine Learning (ICML)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="sun2021adagcn" class="col-sm-8"> <div class="title">AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models</div> <div class="author"> Ke Sun ,  <em>Zhanxing Zhu</em> ,  and  Zhouchen Lin </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CVPR</abbr> </div> <div id="ye2021adversarial" class="col-sm-8"> <div class="title">Adversarial invariant learning</div> <div class="author"> Nanyang Ye ,  Jingxuan Tang ,  Huayu Deng ,  Xiao-Yun Zhou ,  Qianxiao Li ,  Zhenguo Li , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Guang-Zhong Yang, Zhanxing Zhu' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TPAMI</abbr> </div> <div id="xu2021adaptive" class="col-sm-8"> <div class="title">Adaptive progressive continual learning</div> <div class="author"> Ju Xu ,  Jin Ma ,  Xuesong Gao ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>IEEE transactions on pattern analysis and machine intelligence (TPAMI)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TNNLS</abbr> </div> <div id="ye2021annealing" class="col-sm-8"> <div class="title">An annealing mechanism for adversarial training acceleration</div> <div class="author"> Nanyang Ye ,  Qianxiao Li ,  Xiao-Yun Zhou ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>IEEE Transactions on Neural Networks and Learning Systems</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="wan2021spherical" class="col-sm-8"> <div class="title">Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay</div> <div class="author"> Ruosi Wan ,  <em>Zhanxing Zhu</em> ,  Xiangyu Zhang ,  and  Jian Sun </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we comprehensively reveal the learning dynamics of normalized neural network using Stochastic Gradient Descent (with momentum) and Weight Decay (WD), named as Spherical Motion Dynamics (SMD). Most related works focus on studying behavior of effective learning rate "inequilibrium" state, i.e. assuming weight norm remains unchanged. However, their discussion on why this equilibrium can be reached is either absent or less convincing. Our work directly explores the cause of equilibrium, as a special state of SMD. Specifically, 1) we introduce the assumptions that can lead to equilibrium state in SMD, and prove equilibrium can be reached in a linear rate regime under given assumptions; 2) we propose angular update" as a substitute for effective learning rate to depict the state of SMD, and derive the theoretical value of angular update in equilibrium state; 3) we verify our assumptions and theoretical results on various large-scale computer vision tasks including ImageNet and MSCOCO with standard settings. Experiment results show our theoretical findings agree well with empirical observations. We also show that the behavior of angular update in SMD can produce interesting effect to the optimization of neural network in practice.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IJCNN</abbr> </div> <div id="zhang2020learning" class="col-sm-8"> <div class="title">Learning to search efficient densenet with layer-wise pruning</div> <div class="author"> Xuanyang Zhang ,  Hao Liu ,  <em>Zhanxing Zhu</em> ,  and  Zenglin Xu </div> <div class="periodical"> <em>In 2020 International Joint Conference on Neural Networks (IJCNN)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ECML</abbr> </div> <div id="wan2020neural" class="col-sm-8"> <div class="title">Neural control variates for Monte Carlo variance reduction</div> <div class="author"> Ruosi Wan ,  Mingjun Zhong ,  Haoyi Xiong ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, Würzburg, Germany, September 16–20, 2019, Proceedings, Part II</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="sun2020multi" class="col-sm-8"> <div class="title">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes.</div> <div class="author"> Ke Sun ,  Zhouchen Lin ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In AAAI</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="yao2020efficient" class="col-sm-8"> <div class="title">Efficient Neural Architecture Search via Proximal Iterations.</div> <div class="author"> Quanming Yao ,  Ju Xu ,  Wei-Wei Tu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In AAAI</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TOPS</abbr> </div> <div id="ye2020using" class="col-sm-8"> <div class="title">Using generative adversarial networks to break and protect text captchas</div> <div class="author"> Guixin Ye ,  Zhanyong Tang ,  Dingyi Fang ,  <em>Zhanxing Zhu</em> ,  Yansong Feng ,  Pengfei Xu , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Xiaojiang Chen, Jungong Han, Zheng Wang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>ACM Transactions on Privacy and Security (TOPS)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="wu2020noisy" class="col-sm-8"> <div class="title">On the Noisy Gradient Descent that Generalizes as SGD</div> <div class="author"> Jingfeng Wu ,  Wenqing Hu ,  Haoyi Xiong ,  Jun Huan ,  Vladimir Braverman ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference for Machine Learning (ICML)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="ye2020amata" class="col-sm-8"> <div class="title">Amata: An Annealing Mechanism for Adversarial Training Acceleration</div> <div class="author"> Nanyang Ye ,  Qianxiao Li ,  Xiao-Yun Zhou ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In AAAI</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ECAI</abbr> </div> <div id="guo2020simplifying" class="col-sm-8"> <div class="title">Simplifying Graph Attention Networks with Source-Target Separation</div> <div class="author"> Hantao Guo ,  Rui Yan ,  Yansong Feng ,  Xuesong Gao ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In ECAI</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="zhang2020black" class="col-sm-8"> <div class="title">Black-box certification with randomized smoothing: A functional optimization based framework</div> <div class="author"> Dinghuai Zhang ,  Mao Ye ,  Chengyue Gong ,  <em>Zhanxing Zhu</em> ,  and  Qiang Liu </div> <div class="periodical"> <em>In NeurIPS</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="shi2020informative" class="col-sm-8"> <div class="title">Informative Dropout for Robust Representation Learning: A Shape-bias Perspective</div> <div class="author"> Baifeng Shi ,  Dinghuai Zhang ,  Qi Dai ,  <em>Zhanxing Zhu</em> ,  Yadong Mu ,  and  Jingdong Wang </div> <div class="periodical"> <em>In International Conference for Machine Learning (ICML)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI</abbr> </div> <div id="xu2020automatic" class="col-sm-8"> <div class="title">Automatic data augmentation for 3D medical image segmentation</div> <div class="author"> Ju Xu ,  Mengzhang Li ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part I 23</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ACML</abbr> </div> <div id="wu2020towards" class="col-sm-8"> <div class="title">Towards understanding and improving the transferability of adversarial examples in deep neural networks</div> <div class="author"> Lei Wu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Asian Conference on Machine Learning (ACML)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="chen2020neural" class="col-sm-8"> <div class="title">Neural Approximate Sufficient Statistics for Implicit Models</div> <div class="author"> Yanzhi Chen ,  Dinghuai Zhang ,  Michael Gutmann ,  Aaron Courville ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="ji2020knowledge" class="col-sm-8"> <div class="title">Knowledge Distillation in Wide Neural Networks: Risk Bound, Data Efficiency and Imperfect Teacher</div> <div class="author"> Guangda Ji ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://proceedings.neurips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="chen2020breaking" class="col-sm-8"> <div class="title">On breaking deep generative model-based defenses and beyond</div> <div class="author"> Yanzhi Chen ,  Renjie Xie ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="li2020spatial" class="col-sm-8"> <div class="title">Spatial-temporal fusion graph neural networks for traffic flow forecasting</div> <div class="author"> Mengzhang Li ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In AAAI Conference on Artificial Intelligence</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CVPR</abbr> </div> <div id="yu2018tangent" class="col-sm-8"> <div class="title">Tangent-Normal Adversarial Regularization for Semi-supervised Learning</div> <div class="author"> Bing Yu ,  Jingfeng Wu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In CVPR</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="xiong2019sphmc" class="col-sm-8"> <div class="title">SpHMC: Spectral Hamiltonian Monte Carlo</div> <div class="author"> Haoyi Xiong ,  Kafeng Wang ,  Jiang Bian ,  <em>Zhanxing Zhu</em> ,  Cheng-Zhong Xu ,  Zhishan Guo , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jun Huan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In AAAI 2019</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Lancet</abbr> </div> <div id="zou2019novel" class="col-sm-8"> <div class="title">Novel subgroups of patients with adult-onset diabetes in Chinese and US populations</div> <div class="author"> Xiantong Zou ,  Xianghai Zhou ,  <em>Zhanxing Zhu</em> ,  and  Linong Ji </div> <div class="periodical"> <em>The Lancet Diabetes &amp; Endocrinology</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">PRCV</abbr> </div> <div id="sun2019virtual" class="col-sm-8"> <div class="title">Virtual adversarial training on graph convolutional networks in node classification</div> <div class="author"> Ke Sun ,  Zhouchen Lin ,  Hantao Guo ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Pattern Recognition and Computer Vision: Second Chinese Conference, PRCV 2019, Xi’an, China, November 8–11, 2019, Proceedings, Part I 2</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yu20193d" class="col-sm-8"> <div class="title">3D graph convolutional networks with temporal graphs: A spatial information free framework for traffic forecasting</div> <div class="author"> Bing Yu ,  Mengzhang Li ,  Jiyong Zhang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>arXiv preprint arXiv:1903.00919</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yu2019st" class="col-sm-8"> <div class="title">ST-UNet: A spatio-temporal U-network for graph-structured time series modeling</div> <div class="author"> Bing Yu ,  Haoteng Yin ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>arXiv preprint arXiv:1903.05631</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="zhang2019you" class="col-sm-8"> <div class="title">You only propagate once: Accelerating adversarial training via maximal principle</div> <div class="author"> Dinghuai Zhang ,  Tianyuan Zhang ,  Yiping Lu ,  <em>Zhanxing Zhu</em> ,  and  Bin Dong </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin’s Maximum Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5   1/4 GPU time of the projected gradient descent (PGD) algorithm.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="zhang2019interpreting" class="col-sm-8"> <div class="title">Interpreting Adversarially Trained Convolutional Neural Networks</div> <div class="author"> Tianyuan Zhang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="zhu2019anisotropic" class="col-sm-8"> <div class="title">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects</div> <div class="author"> <em>Zhanxing Zhu</em> ,  Jingfeng Wu ,  Bing Yu ,  Lei Wu ,  and  Jinwen Ma </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NLPCC</abbr> </div> <div id="hu2019question" class="col-sm-8"> <div class="title">How question generation can help question answering over knowledge base</div> <div class="author"> Sen Hu ,  Lei Zou ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9–14, 2019, Proceedings, Part I 8</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICDM</abbr> </div> <div id="wan2019towards" class="col-sm-8"> <div class="title">Towards making deep transfer learning never hurt</div> <div class="author"> Ruosi Wan ,  Haoyi Xiong ,  Xingjian Li ,  <em>Zhanxing Zhu</em> ,  and  Jun Huan </div> <div class="periodical"> <em>In 2019 IEEE International Conference on Data Mining (ICDM)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IJCAI</abbr> </div> <div id="yu2018spatio" class="col-sm-8"> <div class="title">Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting</div> <div class="author"> Bing Yu ,  Haoteng Yin ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Joint Conference of Artificial Intelligence (IJCAI)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ijcai.org/proceedings/2018/0505.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="https://dl.acm.org/doi/10.5555/3304222.3304273" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NIPS</abbr> </div> <div id="xu2018reinforced" class="col-sm-8"> <div class="title">Reinforced continual learning</div> <div class="author"> Ju Xu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CCS</abbr> </div> <div id="ye2018yet" class="col-sm-8"> <div class="title">Yet another text captcha solver: A generative adversarial network based approach</div> <div class="author"> Guixin Ye ,  Zhanyong Tang ,  Dingyi Fang ,  <em>Zhanxing Zhu</em> ,  Yansong Feng ,  Pengfei Xu , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Xiaojiang Chen, Zheng Wang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security (ACM CCS)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ISBI</abbr> </div> <div id="yuan2018sipid" class="col-sm-8"> <div class="title">SIPID: A deep learning framework for sinogram interpolation and image denoising in low-dose CT reconstruction</div> <div class="author"> Huizhuo Yuan ,  Jinzhu Jia ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IJCAI</abbr> </div> <div id="ye2018stochastic" class="col-sm-8"> <div class="title">Stochastic Fractional Hamiltonian Monte Carlo.</div> <div class="author"> Nanyang Ye ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In IJCAI</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NIPS</abbr> </div> <div id="luo2018thermostat" class="col-sm-8"> <div class="title">Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning</div> <div class="author"> Rui Luo ,  Jianhong Wang ,  Yaodong Yang ,  Jun Wang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NIPS</abbr> </div> <div id="ye2018bayesian" class="col-sm-8"> <div class="title">Bayesian adversarial learning</div> <div class="author"> Nanyang Ye ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NIPS)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="wu2017towards" class="col-sm-8"> <div class="title">Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes</div> <div class="author"> Lei Wu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In The 34th International Conference on Machine Learning (ICML 2017): Theoretical Machine Learning Workshop</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NIPS</abbr> </div> <div id="ye2017langevin" class="col-sm-8"> <div class="title">Langevin Dynamics with Continuous Tempering for Training Deep Neural Networks</div> <div class="author"> Nanyang Ye ,  <em>Zhanxing Zhu</em> ,  and  Rafal K Mantiuk </div> <div class="periodical"> <em>In 31st Neural Information Processing Systems (NIPS)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ACL</abbr> </div> <div id="luo2017learning" class="col-sm-8"> <div class="title">Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix</div> <div class="author"> Bingfeng Luo ,  Yansong Feng ,  Zheng Wang ,  <em>Zhanxing Zhu</em> ,  Songfang Huang ,  Rui Yan , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Dongyan Zhao' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ACL</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">AAAI</abbr> </div> <div id="zhu2016stochastic" class="col-sm-8"> <div class="title">Stochastic parallel block coordinate descent for large-scale saddle point problems</div> <div class="author"> <em>Zhanxing Zhu</em> ,  and  Amos Storkey </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> , 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ECML</abbr> </div> <div id="zhu2015adaptive" class="col-sm-8"> <div class="title">Adaptive stochastic primal-dual coordinate descent for separable saddle point problems</div> <div class="author"> <em>Zhanxing Zhu</em> ,  and  Amos J Storkey </div> <div class="periodical"> <em>In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15</em> , 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ECML</abbr> </div> <div id="storkey2015aggregation" class="col-sm-8"> <div class="title">Aggregation under bias: Rényi divergence aggregation and its implementation via machine learning markets</div> <div class="author"> Amos J Storkey ,  <em>Zhanxing Zhu</em> ,  and  Jinli Hu </div> <div class="periodical"> <em>In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15</em> , 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NIPS</abbr> </div> <div id="shang2015covariance" class="col-sm-8"> <div class="title">Covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling</div> <div class="author"> Xiaocheng Shang ,  <em>Zhanxing Zhu</em> ,  Benedict Leimkuhler ,  and  Amos J Storkey </div> <div class="periodical"> <em>In NIPS</em> , 2015 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="storkey2014continuum" class="col-sm-8"> <div class="title">A continuum from mixtures to products: Aggregation under bias</div> <div class="author"> A Storkey ,  <em>Zhanxing Zhu</em> ,  and  Jinli Hu </div> <div class="periodical"> <em>In ICML workshop on divergence methods for probabilistic inference</em> , 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NPL</abbr> </div> <div id="zhu2013supervised" class="col-sm-8"> <div class="title">Supervised distance preserving projections</div> <div class="author"> <em>Zhanxing Zhu</em> ,  Timo Similä ,  and  Francesco Corona </div> <div class="periodical"> <em>Neural processing letters</em>, 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zhu2013multiplicative" class="col-sm-8"> <div class="title">Multiplicative updates for learning with stochastic matrices</div> <div class="author"> <em>Zhanxing Zhu</em> ,  Zhirong Yang ,  and  Erkki Oja </div> <div class="periodical"> <em>In Image Analysis: 18th Scandinavian Conference, SCIA 2013, Espoo, Finland, June 17-20, 2013. Proceedings 18</em> , 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IFAC</abbr> </div> <div id="zhu2011local" class="col-sm-8"> <div class="title">Local linear regression for soft-sensor design with application to an industrial deethanizer</div> <div class="author"> <em>Zhanxing Zhu</em> ,  Francesco Corona ,  Amaury Lendasse ,  Roberto Baratti ,  and  Jose A Romagnoli </div> <div class="periodical"> <em>IFAC Proceedings Volumes</em>, 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2010</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="yang2010automatic" class="col-sm-8"> <div class="title">Automatic rank determination in projective nonnegative matrix factorization</div> <div class="author"> Zhirong Yang ,  <em>Zhanxing Zhu</em> ,  and  Erkki Oja </div> <div class="periodical"> <em>In International Conference on Latent Variable Analysis and Signal Separation</em> , 2010 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Zhanxing Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>