<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zhanxing Zhu </title> <meta name="author" content="Zhanxing Zhu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="machine learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhanxingzhu.github.io//"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">group members </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Zhanxing</span> Zhu </h1> <p class="desc">Machine learning researcher.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile_pic-480.webp 480w,/assets/img/profile_pic-800.webp 800w,/assets/img/profile_pic-1400.webp 1400w," sizes="(min-width: 900px) 261.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile_pic.png?ee1209c48a4b9751d67912bb010dd98a" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile_pic.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>ECS, University of Southampton</p> <p>Southampton, SO17 1BJ, UK</p> <p>Email: z.zhu@soton.ac.uk </p> </div> </div> <div class="clearfix"> <p>I am an Associate Professor at Vision, Learning and Control Group (<a href="https://www.southampton.ac.uk/research/groups/vision-learning-control" rel="external nofollow noopener" target="_blank">VLC</a>), School of Electrical and Computer Science (<a href="https://www.southampton.ac.uk/about/faculties-schools-departments/school-of-electronics-and-computer-science" rel="external nofollow noopener" target="_blank">ECS</a>), <a href="https://www.southampton.ac.uk/" rel="external nofollow noopener" target="_blank">University of Southampton</a>, UK. I am now closely affiliated with <a href="https://sustai.info/" rel="external nofollow noopener" target="_blank">UKRI AI Centre for Doctoral Training in AI for Sustainability</a>. Previously I obtained Ph.D on machine learning from School of Informatics, University of Edinburgh, UK.</p> <p>I have been focusing on machine learning, particularly, deep learning, broadly covering its theory, methodology and application. Together with my students and collaborators, we attempt to <strong>rigorously reveal the underlying mechanism of why deep learning works or not</strong>, and inspired by our theoretical understanding and empirical observation, we develop <strong>robust, fast and generalizable models and algorithms</strong> to boost its applicability in various challenging scenarios and <strong>interdisciplinary tasks</strong>, e.g. <strong>AI4Science</strong> and <strong>AI4Engineering</strong>. More information is shown in my <a href="https://scholar.google.com/citations?user=a2sHceIAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a> profile.</p> <h3 id="research-interests">Research Interests:</h3> <ul> <li>Understanding deep learning theoretically: SGD (<a href="http://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf" rel="external nofollow noopener" target="_blank">ICML’19</a>, <a href="https://proceedings.mlr.press/v119/wu20c" rel="external nofollow noopener" target="_blank">ICML’20</a>); Batch Normalization (<a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf" rel="external nofollow noopener" target="_blank">NeurIPS’21</a>); Implicit Bias (<a href="https://openreview.net/pdf?id=PjBEUTVzoe" rel="external nofollow noopener" target="_blank">NeurIPS’23</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34118" rel="external nofollow noopener" target="_blank">AAAI’25</a>); Knowledge Distillation (<a href="https://proceedings.neurips.cc/paper/2020/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf" rel="external nofollow noopener" target="_blank">NeurIPS’20</a>); Adversarial Training (<a href="https://openreview.net/pdf?id=l8It-0lE5e7" rel="external nofollow noopener" target="_blank">ICLR’22</a>, <a href="https://openreview.net/pdf?id=l8It-0lE5e7" rel="external nofollow noopener" target="_blank">TPAMI’25</a> <strong><mark>New!</mark></strong>); Neural Scaling Law of LLMs (<a href="https://openreview.net/forum?id=wYxOMEzpkl" rel="external nofollow noopener" target="_blank">ICLR’25</a> <strong><mark>New!</mark></strong>)</li> <li>Robust deep learning models in adversarial and continual environments: YOPO (<a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf" rel="external nofollow noopener" target="_blank">NeurIPS’19</a>), Inversion Attack (<a href="http://proceedings.mlr.press/v119/chen20w/chen20w.pdf" rel="external nofollow noopener" target="_blank">ICML’20</a>), Adversarial Invariant Learning (<a href="https://ieeexplore.ieee.org/document/9577653" rel="external nofollow noopener" target="_blank">CVPR’21</a>), RCL (<a href="https://proceedings.neurips.cc/paper/2018/file/cee631121c2ec9232f3a2f028ad5c89b-Paper.pdf" rel="external nofollow noopener" target="_blank">NeurIPS’18</a>) and BOCL (<a href="https://ieeexplore.ieee.org/document/9477031" rel="external nofollow noopener" target="_blank">TPAMI’21</a>)</li> <li>Learning from complex dynamics data: (<a href="https://www.ijcai.org/proceedings/2018/0505.pdf" rel="external nofollow noopener" target="_blank">STGCN</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/download/16542/16349" rel="external nofollow noopener" target="_blank">STFGN</a>, <a href="https://openreview.net/pdf?id=bISkJSa5Td" rel="external nofollow noopener" target="_blank">Neural Lad</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0004370224000948" rel="external nofollow noopener" target="_blank">Functional Relation Field</a>, <a href="https://openreview.net/forum?id=WjDjem8mWE&amp;referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)" rel="external nofollow noopener" target="_blank">DyCAST</a> <strong><mark>New!</mark></strong>)</li> <li>AI for Science (AI4Science), including climate, material and healthcare problems (<a href="https://openreview.net/forum?id=wYxOMEzpkl" rel="external nofollow noopener" target="_blank">Unisoma for Multi-Solid Simulation</a> <strong><mark>New!</mark></strong> <a href="https://openreview.net/forum?id=wYxOMEzpkl" rel="external nofollow noopener" target="_blank">LaDEEP for Simulating Elastic-Plastic Solids</a> <strong><mark>New!</mark></strong>).</li> </ul> <p><strong><mark>Ph.D Studentships.</mark></strong> I’m interested in supervising motivated students in the area of AI and machine learning, ranging from theory, algorithms and various applications. Please get in touch to discuss the options and potential topics. You can also check out the <a href="https://sustai.info/" rel="external nofollow noopener" target="_blank">UKRI AI Centre for Doctoral Training in AI for Sustainability</a> which has opportunities for 70 PhD students in the area of AI and environmental sustainability.</p> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">TPAMI</abbr> </div> <div id="lyu2025analyzing" class="col-sm-8"> <div class="title">Analyzing the implicit bias of adversarial training from a generalized margin perspective</div> <div class="author"> Bochen Lyu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Adversarial training has been empirically demonstrated as an effective strategy to improve the robustness of deep neural networks (DNNs) against adversarial examples. However, the underlying reason of its effectiveness is still non-transparent. In this paper we conduct both extensive theoretical and empirical analysis on the implicit bias induced by adversarial training from a generalized margin perspective. Our results focus on adversarial training for homogeneous DNNs. In particular, (i). For deep linear networks with \ell_p-norm perturbation, we show that weight matrices of adjacent layers get aligned and the converged parameters maximize the margin of adversarial examples, which can be further viewed as a generalized margin of the original dataset that can be achieved by an interpolation solution between \ell_2-SVM and \ell_q-SVM where 1/p + 1/q=1. (ii). For general homogeneous DNNs, \colorblack including both linear and nonlinear ones, we investigate adversarial training with a variety of adversarial perturbations in a unified manner. Specifically, we show that the direction of the limit point of parameters converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. \colorblackAdditionally, as an application of this general result for two special linear homogeneous DNNs, diagonal linear networks and linear convolutional networks, we show that adversarial training with \ell_p-norm perturbation equivalently minimizes an interpolation norm that depends on the depth, the architecture, and the value of p in the predictor space. Extensive experiments are conducted to verify theoretical claims. Our results theoretically provide the basis for the longstanding folklore \citeAT that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="lyu2025solvable" class="col-sm-8"> <div class="title">A Solvable Attention for Neural Scaling Laws</div> <div class="author"> Bochen Lyu ,  Di Wang ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Conference on Learning Representation (ICLR)</em> , 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/forum?id=wYxOMEzpkl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Transformers and many other deep learning models are empirically shown to predictably enhance their performance as a power law in training time, model size, or the number of training data points, which is termed as the neural scaling law. This paper studies this intriguing phenomenon particularly for the transformer architecture in theoretical setups. Specifically, we propose a framework for self-attention, the underpinning block of transformer, to learn in an in-context manner, where the corresponding learning dynamics is modeled as a non-linear ordinary differential equation (ODE) system. Furthermore, we establish a procedure to derive a tractable solution for this ODE system by reformulating it as a Riccati equation, which allows us to precisely characterize neural scaling laws for self-attention with training time, model size, data size, and the optimal compute. In addition, we reveal that the self-attention shares similar neural scaling laws with several other architectures when the context sequence length of the in-context learning is fixed, otherwise it would exhibit a different scaling law of training time.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="lyu2023implicit" class="col-sm-8"> <div class="title">Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network</div> <div class="author"> Bochen Lyu ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=PjBEUTVzoe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Studying the implicit bias of gradient descent (GD) and stochastic gradient descent (SGD) is critical to unveil the underlying mechanism of deep learning. Unfortunately, even for standard linear networks in regression setting, a comprehensive characterization of the implicit bias is still an open problem. This paper proposes to investigate a new proxy model of standard linear network, rank-1 linear network, where each weight matrix is parameterized as a rank-1 form. For over-parameterized regression problem, we precisely analyze the implicit bias of GD and SGD—by identifying a “potential” function such that GD converges to its minimizer constrained by zero training error (i.e., interpolation solution), and further characterizing the role of the noise introduced by SGD in perturbing the form of this potential. Our results explicitly connect the depth of the network and the initialization with the implicit bias of GD and SGD. Furthermore, we emphasize a new implicit bias of SGD jointly induced by stochasticity and over-parameterization, which can reduce the dependence of the SGD’s solution on the initialization. Our findings regarding the implicit bias are different from that of a recently popular model, the diagonal linear network. We highlight that the induced bias of our rank-1 model is more consistent with standard linear network while the diagonal one is not. This suggests that the proposed rank-1 linear network might be a plausible proxy for standard linear net.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="wan2021spherical" class="col-sm-8"> <div class="title">Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay</div> <div class="author"> Ruosi Wan ,  <em>Zhanxing Zhu</em> ,  Xiangyu Zhang ,  and  Jian Sun </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we comprehensively reveal the learning dynamics of normalized neural network using Stochastic Gradient Descent (with momentum) and Weight Decay (WD), named as Spherical Motion Dynamics (SMD). Most related works focus on studying behavior of effective learning rate "inequilibrium" state, i.e. assuming weight norm remains unchanged. However, their discussion on why this equilibrium can be reached is either absent or less convincing. Our work directly explores the cause of equilibrium, as a special state of SMD. Specifically, 1) we introduce the assumptions that can lead to equilibrium state in SMD, and prove equilibrium can be reached in a linear rate regime under given assumptions; 2) we propose angular update" as a substitute for effective learning rate to depict the state of SMD, and derive the theoretical value of angular update in equilibrium state; 3) we verify our assumptions and theoretical results on various large-scale computer vision tasks including ImageNet and MSCOCO with standard settings. Experiment results show our theoretical findings agree well with empirical observations. We also show that the behavior of angular update in SMD can produce interesting effect to the optimization of neural network in practice.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div> <div id="zhang2019you" class="col-sm-8"> <div class="title">You only propagate once: Accelerating adversarial training via maximal principle</div> <div class="author"> Dinghuai Zhang ,  Tianyuan Zhang ,  Lu ,  <em>Zhanxing Zhu</em> ,  and  Bin Dong </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems (NeurIPS)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin’s Maximum Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5   1/4 GPU time of the projected gradient descent (PGD) algorithm.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="zhu2019anisotropic" class="col-sm-8"> <div class="title">The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects</div> <div class="author"> <em>Zhanxing Zhu</em> ,  Jingfeng Wu ,  Bing Yu ,  Lei Wu ,  and  Jinwen Ma </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v97/zhu19e/zhu19e.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IJCAI</abbr> </div> <div id="yu2018spatio" class="col-sm-8"> <div class="title">Spatio-temporal graph convolutional neural network: A deep learning framework for traffic forecasting</div> <div class="author"> Bing Yu ,  Haoteng Yin ,  and  <em>Zhanxing Zhu</em> </div> <div class="periodical"> <em>In International Joint Conference of Artificial Intelligence (IJCAI)</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ijcai.org/proceedings/2018/0505.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="https://dl.acm.org/doi/10.5555/3304222.3304273" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Timely accurate traffic forecast is crucial for urban traffic control and guidance. Due to the high nonlinearity and complexity of traffic flow, traditional methods cannot satisfy the requirements of mid-and-long term prediction tasks and often neglect spatial and temporal dependencies. In this paper, we propose a novel deep learning framework, Spatio-Temporal Graph Convolutional Networks (STGCN), to tackle the time series prediction problem in traffic domain. Instead of applying regular convolutional and recurrent units, we formulate the problem on graphs and build the model with complete convolutional structures, which enable much faster training speed with fewer parameters. Experiments show that our model STGCN effectively captures comprehensive spatio-temporal correlations through modeling multi-scale traffic networks and consistently outperforms state-of-the-art baselines on various real-world traffic datasets.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A.%7A%68%75@%73%6F%74%6F%6E.%61%63.%75%6B" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=a2sHceIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/zhanxingzhu" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">The best way to approach me is email. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Zhanxing Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>